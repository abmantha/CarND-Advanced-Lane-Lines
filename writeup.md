# Advanced Lane Finding Project

The goals / steps of this project are the following:

* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
* Apply a distortion correction to raw images.
* Use color transforms, gradients, etc., to create a thresholded binary image.
* Apply a perspective transform to rectify binary image ("birds-eye view").
* Detect lane pixels and fit to find the lane boundary.
* Determine the curvature of the lane and vehicle position with respect to center.
* Warp the detected lane boundaries back onto the original image.
* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.

[//]: # (Image References)

[image1]: ./output_images/example_undistorted_calibration_images.png
[image2]: ./output_images/example_undistorted_test_images.png
[image3]: ./output_images/color_gradient_thresholding_plot.png
[image4]: ./output_images/example_undistorted_warped_test_images.png
[image5]: ./output_images/example_undistorted_warped_test_image_single.png
[image6]: ./output_images/test_image_0_detected_lanes.png
[image7]: ./output_images/detected_lanes.png
[image8]: ./output_images/reviewer-notes-1.png
[image9]: ./output_images/reviewer-notes-2.png
[image10]: ./output_images/reviewer-notes-3.png

## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points

### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

Woohoo :)

---

__UPDATE 3__: I have successfully made all recommendations identified by my second submission reviewer. Here are images of his review that I specifically focused and made changes to in my code: 

![alt text][image8]
![alt text][image9]
![alt text][image10]

### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  [Here](https://github.com/udacity/CarND-Advanced-Lane-Lines/blob/master/writeup_template.md) is a template writeup for this project you can use as a guide and a starting point.  

You're reading it!

### Camera Calibration

#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.

The code for this step is contained in the first code cell of the IPython notebook located in P4.ipynb under Section 1, Camera Calibration. Almost identical to the process shown in lecture, I create and populate two lists, objpoints and imgpoints. objpoints is responsible for holding (x, y) coordinates that correspond to the actual location of the chessboard corners found in the world (or 3D space), while imgpoints represents those (x, y, z) coordinates mapped to 2D-space -- they don't have an associated z-value, just two parameters x and y; imgpoints will be updated with newly found (x, y) pixel positions of successful chessboard detections. Each calibration image is converted to grayscale and is passed to cv2.findChessboardCorners() which finds chessboard corners located in 2D space. For each image, I simply append newly discovered corners and objp points to imgpoints and objpoints respectively. 

After creating these two lists, I can now proceed to generating important calibration matrices that will allow my program to remove any distortion found within a given image by calling cv2.calibrateCamera(). This code is located in the second and third cells of my IPython notebook. It's important to note that this input image must be converted to grayscale for the method to properly work.

Finally, I can now remove any distortion within an image by simply calling cv2.undistort() with the calibration matrices generated by cv2.calibrateCamera().

I start by preparing "object points", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  

I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result: 

![alt text][image1]

This code is found under the section "Example of Undistortion on Test Images"

### Pipeline (single images)

#### 1. Provide an example of a distortion-corrected image.

I then applied the same cv2.undistort() method on the 6 test images provided. Here's a before-and-after shot of all 6:

![alt text][image2]

This code is found under the section "Example of Undistortion on Test Images". I can use any image in order to retrieve the two important calibration matrices (mtx and dist). For this assignment, I simply used the first calibration image pictured above, in addition to objpoints and imgpoints, to generate the mtx and dist matrices. It is only necessary to generate these matrices and calibration points once. 

#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.

In the code cell, "4. Color/Gradient Thresholding," I have 4 helpful methods that allow me to easily test different combinations of color and gradient thresholds: abs_sobel_threshold, magnitude_threshold, direction_threshold, and binary_color_threshold. abs_sobel_threshold, magnitude_threshold and direction_threshold calculate and apply gradient thresholds to a given image, where as binary_color_threshold applies a single color threshold to a given image. Because thresholds are really just Numpy arrays, I can easily combine thresholds together to form more powerful ways to select features of an image that I care about. 

In the cell below, "Testing Threshold Techniques," this is where I test different combinations of color and gradient thresholds on images and their transforms. Here's an example image of a color+gradient threshold that uses an HLS and HSV color threshold combined with an x-oriented gradient threshold that uses the Sobel operator:

![alt text][image3]

Please note that I conduct thresholds on images that have been transformed via perspective transform, which I will explain shortly. 

#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.

In the code cell, "3. Perspective Transform," I have a method called warp() which takes in an image, source and destination arrays. The given input will undergo a perspective transform, transforming images in a source region to a new destination region. It will return the warped image, a matrix M that is responsible for the transformation from source to destination, and an inverse matrix Minv that is responsible for the transformation from destination to source. 

__UPDATE__: The thresholds that I apply consist of a color threshold combination of LAB and LUV binary color thresholds combined with an x-oriented gradient threshold. This threshold combination does a good job of picking up both yellow and white lines. 

These are the specific channels and parameters I used: 
 ```python
    lab_binary_threshold = binary_color_threshold(image, cv2.COLOR_RGB2Lab, 2, (145, 255))
    luv_binary_threshold = binary_color_threshold(image, cv2.COLOR_RGB2Luv, 0, (207, 255))    
    sobel_binary_x = abs_sobel_threshold(image, 'x', 3, (20, 100))

    combined_color_binary = cv2.bitwise_or(lab_binary_threshold, luv_binary_threshold)
    combined_binary = np.zeros_like(sobel_binary_x)
    combined_binary[(sobel_binary_x == 1) | (combined_color_binary == 1)] = 1
 ```
 
The B channel of LAB does a great job identifying yellow lines. The L channel of LUV does a decent job identifying white lines. Thresholding with an x-oriented gradient allowed me to add in additionally identified white, right line points that made it possible to easily detect centroid values.

In the next code cell, "Identify source and destination points on test images," I explore different source and destination points that will generate the right transform that I'm looking for to capture lane lines as close to parallel as possible in a bird's-eye view. Here are all 6 test images after being transformed. 

![alt text][image4]

I hardcoded the source and destination points, and came upon my final choice via trial and error. Below is the snippet of code that I used to select both source and destination points. 

__UPDATE 3__:
Updating the source and destination for the perspective transform significantly improve my pipeline. I now had an appropriate bird's-eye view of the lanes and was able to easily detect both yellow and white lane markings.

```python
    src = np.float32([(570,470), (750,470), (200,680), (1157,680)])
    dest = np.float32([(250,0), (1100,0), (250,720), (1100,720)])
```

This resulted in the following source and destination points:

| Source        | Destination   | 
|:-------------:|:-------------:| 
| 570, 470      | 250, 0        | 
| 750, 470      | 1100, 0       |
| 200, 680      | 250, 0        |
| 1157, 680     | 1100, 720     |

I verified that my perspective transform was working as expected by drawing 4 green dots on the regions of interest. The 4 dots (green, blue, orange and red) allow me to easily check that the endpoints of the region of interest (source) get appropriately mapped to the region that I want (destination). (Please note: dots were removed for submission).

For my second submission, identifying the correct source and destinations significantly improved my pipeline's performance. One of the issues with my previous submission was that the warped images did not have parallel lane lines which resulted in not enough line segments to be detected. This issue is now resolved.

![alt text][image5]

#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?

My code to identify lane-line pixels and their polynomials is in the code cell "5. Lane Finding: Sliding Window Search w/ Convolutions." I approached finding the lane-lines using the convolutional search approach recommend in lecture as an alternative to traditional sliding window search. I primarily decided to this out of curiosity of alternative applications of convolutions other than in deep nets. 

The basic idea is to first compute a histogram of potential starting locations in the bottom of a given warped and thresholded image. We run over a generated histogram of "hot" and "cold" points, points that could or could not be lane lines, respectively, using the np.convolve() operation. The goal is to effectively travel up a series of convolutional windows along what most likely is a lane line by following white pixels generated after thresholding and transforming. Lanes that are generally close to parallel and have an equivalent amount of separation from top to bottom means that we've probably found a good detection for a lane line. Once we do, we simply append these centroid centers to a list that is maintained during moving video stream. 

I introduce a margin value that allows the algorithm to account for slight deviations of detection in the image. The lower the margin, the more biased the algorithm will be towards detecting curves. The higher the margin, the more likely the algorithm will succumb to too many false detections, or deviations, resulting in jumpiness and erratic centroid center detection.  

![alt text][image6]

#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.

In the same cell of the notebook as above, I also calculate the radius of curvature and the position of the vehicle with respect to center. I accomplish this with 2 methods called radius_of_curvature() and distace_from_center(). The implementations for both methods are found in the code cell "Measuring Radius of Curvature." 

To calculate radius of curvature, I simply transformed the converted left and right polynomial fits to be in meters instead of pixels using a conversion of y_meters_per_pixel = 30 / 720 and x_meters_per_pixel = 3.7 / 800 with a basic formula that relates radius of curvature to derivatives of position.

To calculate distance from center, I simply took the difference of the center of the image and the difference of both left and right polynomial fits. Finally, I converted this difference from pixels to meters using the same x_meters_per_pixel constant above. 

Once I calculate these values, I then display them on directly onto the video. 

#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.

I implemented this in the function called single_image_convolution_search() in the cell "5. Lane Finding: Sliding Window Search w/ Convolutions" on line 44. 

![alt text][image7]

---

### Pipeline (video)

#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).

Here's a [link to my video result](./project_video_with_lanes.mp4)

UPDATE: I also attempted the challenge videos. Output videos can be found [here](./test_videos/challenge_video_1_with_lanes.mp4), [here](./test_videos/challenge_video_2_with_lanes.mp4) and [here](./test_videos/harder_challenge_video_with_lanes.mp4). I was sadly only successful on challenge_video_1_with_lanes.mp4 :(

---

### Discussion

#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?

My pipeline will likely fail at sharp turns and when driving over light-brown and shady patches of road. I think this is most likely a consequence of the color and gradient thresholding combinations that I have chosen. One of the key issues that I had is that any relatively sharp turn would cause the right fit line to immediately jump to the left because there's not enough white line segments being detected. This resulted in playing around with different values for the margin parameter of the convolution search because that dictates how much flexibility the line detector has in shifting subsequent boxes during vertical sliding search. 

__UPDATE 2__: Because of my updates for color and gradient thresholding, my pipeline now performs very well on the project video. However, one issue is that around the 28-30 second mark, my pipeline mistakes the black car that enters into the frame as part of the lane. When I stepped through the frame data around this point, it was quickly apparent that the car would be identified as a starting centroid location. I think this issue generalizes to when any white object (or any object that is detected as white) other than a car is identified. When I analyzed the warped thresholded images, I noticed that this was definitely the case. I attempted to restrict the centroid search space at the bottom of the image to not exceed a zone greater than 100 + the midpoint of the image in the x-axis. This, however, caused additional issues with other parts of the video, particularly around sharper turns. One thing that I think might fix this situation is to implement a way to update warp perspectives in real-time. Right now, I am comparing centroids that are recorded for each line detection per frame. Instead, I think it'd be a really cool approach to also implement a sanity check based on warped images differences too. If a warped image did not fall into a particular threshold of similarity, then a new set of points would have to be used in order to generate a more accurate region of interest. 

__UPDATE 3__: I believe significant updates to my work pipeline. Now, my pipeline is consistently able to detect the white and yellow lanes without succumbing to noise such as shadows, gray pavements or vehicles in adjacent lanes. I think there's still for improvement however. I did impelement a basic sanity check that simply compared previous and current polynomial detections, so that new detections would only be accepted if they were within a reasonable bound. However, I did have some difficulty extending this. This is certainly something that I would like to return to tackling in the future.

What did work, however, is the convolution search approach. I definitely preferred this technique rather than traditional sliding search mostly of curious for alternative applications of convolutions. I definitely enjoyed seeing these in action, and more than anything it solidified my understanding of convolutions as just being really powerful mathematical operators. 

All in all, I definitely enjoyed this project. It certainly was tricky, but I think I gained a lot out of it!
